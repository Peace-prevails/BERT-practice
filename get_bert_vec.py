# -*- coding: utf-8 -*-
"""get_book_vec.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19-lyBFtP3VdTT8e1xvzk7O8pooS8i5V9
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install transformers
# %tensorflow_version 2.x
import tensorflow as tf
print("Tensorflow version " + tf.__version__)

# try:
#   tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection
#   print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])
# except ValueError:
#   raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')

# tf.config.experimental_connect_to_cluster(tpu)
# tf.tpu.experimental.initialize_tpu_system(tpu)
# tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)



from transformers import BertTokenizer, TFBertModel
from transformers import BertTokenizer
import tensorflow as tf
from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np
from transformers import TFBertForSequenceClassification
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense,Dropout
from tqdm import trange
import pickle
tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
bert_layer = TFBertModel.from_pretrained('bert-base-chinese',num_labels=10)
def split_dataset(df):
    train_set, x = train_test_split(df, 
        stratify=df['label'],
        test_size=0.1, 
        random_state=42)
    val_set, test_set = train_test_split(x, 
        stratify=x['label'],
        test_size=0.5, 
        random_state=43)

    return train_set,val_set, test_set

path="/content/drive/MyDrive/books_df.pkl"
df_raw=pickle.load(open(path,"rb"))
print(df_raw)

sentences=np.array(df_raw['text'])
sentence_vector=[]
for s in trange(0,len(sentences)):
    inputs = tokenizer(sentences[s], return_tensors="tf")
    # print(sentences[s])
    outputs = bert_layer(inputs)
    last_hidden_states = outputs.last_hidden_state
    last_hidden_states=np.array((last_hidden_states))
    sentence_vector.append(last_hidden_states[0][0])

print(np.array(sentence_vector).shape)
output_path='/content/drive/MyDrive/xxx_sentence_vector.pkl'
output = open(output_path, 'wb')
pickle.dump(np.array(sentence_vector),output)
output.close()