# -*- coding: utf-8 -*-
"""pure_bert.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15MJgwfrAExatxwsX9UxsW9Ym1Emz5wZQ
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install transformers
# %tensorflow_version 2.x
import tensorflow as tf
print("Tensorflow version " + tf.__version__)

try:
  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection
  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])
except ValueError:
  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')

tf.config.experimental_connect_to_cluster(tpu)
tf.tpu.experimental.initialize_tpu_system(tpu)
tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)
from os import truncate
from transformers import BertTokenizer
import tensorflow as tf
from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np
from transformers import TFBertForSequenceClassification,TFBertModel
from transformers import RobertaTokenizer, TFRobertaForSequenceClassification

tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
# tokenizer = RobertaTokenizer.from_pretrained('roberta-base')

max_length = 25 #一句句子的最大长度
test_sentence = '苹果春季发布会将发布搭载全新芯片的平板'
batch_size=8


def split_dataset(df):
    train_set, x = train_test_split(df, 
        stratify=df['label'],
        test_size=0.975, 
        random_state=42)
    val_set, test_set = train_test_split(x, 
        stratify=x['label'],
        test_size=0.975, 
        random_state=43)

    return train_set,val_set, test_set


df_raw = pd.read_csv("/content/drive/MyDrive/data.txt",sep="\t",header=None,names=["text","label"])    
# label
df_label = pd.DataFrame({"label":["财经","房产","股票","教育","科技","社会","时政","体育","游戏","娱乐"],"y":list(range(10))})
df_raw = pd.merge(df_raw,df_label,on="label",how="left")

train_data,val_data, test_data = split_dataset(df_raw)
print(np.array(train_data).shape)
print(np.array(val_data).shape)
def convert_example_to_feature(review):
    return tokenizer.encode_plus(review, 
                                 add_special_tokens = True, # add [CLS], [SEP]
                                 max_length = 20, # max length of the text that can go to BERT
                                 padding = 'max_length', # add [PAD] tokens
                                 return_attention_mask = True, # add attention mask to not focus on pad tokens
                                 truncation=True
                                )

# map to the expected input to TFBertForSequenceClassification, see here 
def map_example_to_dict(input_ids, attention_masks, token_type_ids, label):
    return {
      "input_ids": input_ids,
      "token_type_ids": token_type_ids,
      "attention_mask": attention_masks,
  },label

def encode(ds, limit=-1):
    # prepare list, so that we can build up final TensorFlow dataset from slices.
    input_ids_list = []
    token_type_ids_list = []
    attention_mask_list = []
    label_list = []
    if (limit > 0):
        ds = ds.take(limit)

    for index, row in ds.iterrows():
        review = row["text"]
        label = row["y"]
        bert_input = convert_example_to_feature(review)

        
        input_ids_list.append(bert_input['input_ids'])
        token_type_ids_list.append(bert_input['token_type_ids'])
        attention_mask_list.append(bert_input['attention_mask'])
        label_list.append([label])

    input_ids=np.array(input_ids_list)
    attention_masks=np.array(attention_mask_list)
    labels=np.array(label_list)
      
    return input_ids,attention_masks,labels
def encode_prediction(review):
    # prepare list, so that we can build up final TensorFlow dataset from slices.
    input_ids_list = []
    token_type_ids_list = []
    attention_mask_list = []
 
    bert_input = convert_example_to_feature(review)

        
    input_ids_list.append(bert_input['input_ids'])
    token_type_ids_list.append(bert_input['token_type_ids'])
    attention_mask_list.append(bert_input['attention_mask'])
        

    input_ids=np.array(input_ids_list)
    attention_masks=np.array(attention_mask_list)
    
      
    return input_ids,attention_masks

print("Encoding processing")
# train dataset

train_inp,train_mask,train_label = encode(train_data)
val_inp,val_mask,val_label=encode(val_data)
test_inp,test_mask,test_label=encode(test_data)
pre_inp,pre_mask=encode_prediction(test_sentence)

print("train_inp length: {}, train_mask length: {}".format(train_inp.shape,train_mask.shape))
print("train_label length: {}".format(train_label.shape))
print("val_inp length: {}, val_label length: {}".format(val_inp.shape,val_label.shape))

# recommended learning rate for Adam 5e-5, 3e-5, 2e-5
learning_rate = 2e-5
# we will do just 1 epoch for illustration, though multiple epochs might be better as long as we will not overfit the model
number_of_epochs = 1
with tpu_strategy.scope(): # creating the model in the TPUStrategy scope means we will train the model on the TPU
# # model initialization
  model = TFBertForSequenceClassification.from_pretrained('bert-base-chinese', num_labels=10)

# # optimizer Adam recommended
  optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate,epsilon=1e-08)
# # we do not have one-hot vectors, we can use sparce categorical cross entropy and accuracy
  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
  metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')
# model.layers[0].trainable = False
  
  model.compile(optimizer=optimizer, loss=loss, metrics=[metric])

print(model.summary())
# # fit model
print("Training process")
bert_history = model.fit([train_inp,train_mask],train_label, batch_size=32,epochs=number_of_epochs,validation_data=([val_inp,val_mask],val_label))

test_sentence="苹果公司秋季再次发布新笔记本。"
inputs = tokenizer(test_sentence, return_tensors="tf")

# sentence_vector.append(last_hidden_states[0][0])
# # # evaluate test set
# model.evaluate([test_inp,test_mask],test_label)

# model.predict([pre_inp,pre_mask])

# predict_temp=model.predict([val_inp,val_mask])
# y_test_pred=np.argmax(predict_temp,axis=1)
# print(y_test_pred.shape)

# y_test=val_label
# from sklearn.metrics import accuracy_score
# from sklearn.metrics import recall_score
# from sklearn.metrics import precision_score
# from sklearn.metrics import f1_score    
# nn_train_acc = accuracy_score(y_test, y_test_pred)
# nn_train_p=precision_score(y_test, y_test_pred,average='macro')
# nn_train_r=recall_score(y_test, y_test_pred,average='macro')
# nn_train_f1=f1_score(y_test, y_test_pred,average='macro')
# print("acc: ",nn_train_acc)
# print("precision: ",nn_train_p)
# print("recall: ",nn_train_r)
# print("f1: ",nn_train_f1)
#model.save("test_model.pkl")

o